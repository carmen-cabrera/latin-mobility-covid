{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "514c37fa",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e1d2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9b99fa",
   "metadata": {},
   "source": [
    "# Define country and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fd6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target country\n",
    "country = 'Colombia'\n",
    "\n",
    "# Set country-specific parameters: ISO codes and buffer size (in meters)\n",
    "if country == 'Argentina':\n",
    "    country_short = 'ARG'   # ISO 3-letter code\n",
    "    country_code = 'AR'     # ISO 2-letter code\n",
    "elif country == 'Chile':\n",
    "    country_short = 'CHL'\n",
    "    country_code = 'CL'\n",
    "elif country == 'Colombia':\n",
    "    country_short = 'COL'\n",
    "    country_code = 'CO'\n",
    "# Uncomment the following if Mexico is to be included in the analysis\n",
    "# elif country == 'Mexico':\n",
    "#     country_short = 'MEX'\n",
    "#     country_code = 'MX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf765d9",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d3914b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory path\n",
    "wd = (\n",
    "    '/your/path/to/working/directory/'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db89ff5",
   "metadata": {},
   "source": [
    "# Load baseline imputed data and corresponding grid files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c46990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geopackages with imputed baseline Facebook population and movement data\n",
    "popcell_baseline_imput_pop = gpd.read_file(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'outputs', country_short,\n",
    "        'grids-with-data/popcell-baseline-imput-pop/popcell-baseline-imput-pop.gpkg'\n",
    "    )\n",
    ")\n",
    "\n",
    "movcell_baseline_imput_pop = gpd.read_file(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'outputs', country_short,\n",
    "        'grids-with-data/movcell-baseline-imput-pop/movcell-baseline-imput-pop.gpkg'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Load baseline movement imputations CSV, excluding the unnecessary column\n",
    "baseline_mov_imput = pd.read_csv(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'outputs', country_short,\n",
    "        'baseline/movcell-baseline-imput-mov-dist-with-exo-var-flatten.csv'\n",
    "    )\n",
    ").drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# Load population grid shapefile and reproject to WGS84 (EPSG:4326)\n",
    "grid_popcell = gpd.read_file(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'inputs', 'grids',\n",
    "        f'Grid_{country}_FB_pop', f'Grid_{country}.shp'\n",
    "    )\n",
    ").to_crs('EPSG:4326')\n",
    "\n",
    "# Load movement grid shapefile and reproject to WGS84\n",
    "grid_movcell = gpd.read_file(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'inputs', 'grids',\n",
    "        f'Grid_{country}_FB_mov', f'Grid_{country}.shp'\n",
    "    )\n",
    ").to_crs('EPSG:4326')\n",
    "\n",
    "# Load lookup table linking movement grid cells to population grid cells\n",
    "grid_lookup = gpd.read_file(\n",
    "    os.path.join(\n",
    "        wd, 'data', 'inputs', 'grids',\n",
    "        f'Grid_{country}_lookup_mov_to_pop.gpkg'\n",
    "    )\n",
    ").to_crs('EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe8ea44",
   "metadata": {},
   "source": [
    "# Create empty DataFrame for population data over time and spatial units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5131648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define directory path containing population data files\n",
    "directory = os.path.join(wd, 'data', 'outputs', country_short, 'pop')\n",
    "\n",
    "# List and sort files in the directory, excluding hidden files (starting with '.')\n",
    "files = sorted([file for file in os.listdir(directory) if not file.startswith('.')])\n",
    "\n",
    "# Extract start and end dates from the filenames assuming date format in last 19 to 9 chars\n",
    "start_date = datetime.strptime(files[0][-19:-9], '%Y-%m-%d')\n",
    "end_date = datetime.strptime(files[-1][-19:-9], '%Y-%m-%d')\n",
    "\n",
    "# Calculate the total number of days between start and end dates\n",
    "delta = end_date - start_date\n",
    "\n",
    "# Create list of date strings for all days between start and end dates\n",
    "columns = [str(start_date + timedelta(days=i))[:10] for i in range(delta.days + 1)]\n",
    "\n",
    "# Initialise DataFrame with NaNs, columns are dates, rows correspond to baseline population grid cells\n",
    "df_pop_evo_popcell = pd.DataFrame({\n",
    "    column: [np.nan] * len(popcell_baseline_imput_pop)\n",
    "    for column in columns\n",
    "})\n",
    "\n",
    "# Insert 'FID' column at the start with index values from baseline population GeoDataFrame\n",
    "df_pop_evo_popcell.insert(\n",
    "    loc=0,\n",
    "    column='FID',\n",
    "    value=popcell_baseline_imput_pop.index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b0329a",
   "metadata": {},
   "source": [
    "# Populate DataFrame with crisis pop data for each date and spatial unit\n",
    "Two options:<br>\n",
    "    - with raw data (reported) only<br>\n",
    "    - with both raw and estimated data from imputed baseline when raw data is missing <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216577b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True or False according to desired option (see description above)\n",
    "raw = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb288220",
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw == True:\n",
    "    \n",
    "    # Iterate over all files containing daily population data\n",
    "    for i in range(len(files)):\n",
    "\n",
    "        # Print progress percentage every 20 files processed to track progress\n",
    "        if i % 20 == 0:\n",
    "            print(i / len(files) * 100)\n",
    "\n",
    "        file = files[i]\n",
    "\n",
    "        # Read the CSV file for the current date and remove the unnamed index column\n",
    "        df_pops = pd.read_csv(os.path.join(directory, file)).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "        date = file[-19:-9]\n",
    "        wday = datetime.strptime(date, \"%Y-%m-%d\").weekday()\n",
    "\n",
    "        # Loop through each row of the current population data file\n",
    "        for j in range(len(df_pops)):\n",
    "            # Get the unique identifier for the population cell (popcell)\n",
    "            FID_popcell = df_pops.loc[j, 'FID']\n",
    "\n",
    "            # Get the crisis population count for this popcell on this date\n",
    "            n_crisis = df_pops.loc[j, 'n_crisis']\n",
    "\n",
    "            # Update the master DataFrame with the crisis population count for the corresponding date\n",
    "            # Find the index in df_pop_evo_popcell where 'FID' matches FID_popcell\n",
    "            df_pop_evo_popcell.loc[\n",
    "                np.where(df_pop_evo_popcell['FID'] == FID_popcell)[0],\n",
    "                str(date)\n",
    "            ] = n_crisis\n",
    "\n",
    "    # After processing all files, save the populated DataFrame to a CSV file for later use\n",
    "    df_pop_evo_popcell.to_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_popcell_raw.csv')\n",
    "    )\n",
    "    \n",
    "else:\n",
    "\n",
    "    # Loop through all files to process daily population data\n",
    "    for i in range(len(files)):\n",
    "        # Print progress every 20 files (~5%)\n",
    "        if i % 20 == 0:\n",
    "            print(f\"{(i / len(files)) * 100:.2f}% completed\")\n",
    "\n",
    "        file = files[i]\n",
    "\n",
    "        # Load population data for the current day and drop unnecessary column\n",
    "        df_pops = pd.read_csv(os.path.join(directory, file)).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "        # Extract date and corresponding weekday from filename\n",
    "        date = file[-19:-9]\n",
    "        wday = datetime.strptime(date, \"%Y-%m-%d\").weekday()\n",
    "\n",
    "        # Iterate through each row (popcell) in the daily data\n",
    "        for j in range(len(df_pops)):\n",
    "            FID_popcell = df_pops.loc[j, 'FID']\n",
    "            n_crisis = df_pops.loc[j, 'n_crisis']\n",
    "\n",
    "            if not pd.isna(n_crisis):\n",
    "                # If crisis population data exists, update dataframe directly\n",
    "                df_pop_evo_popcell.loc[\n",
    "                    np.where(df_pop_evo_popcell['FID'] == FID_popcell)[0], str(date)\n",
    "                ] = n_crisis\n",
    "            else:\n",
    "                # If crisis data is missing, estimate using baseline and percent change\n",
    "                per_change = df_pops.loc[j, 'percent_change']\n",
    "                n_baseline = popcell_baseline_imput_pop.loc[\n",
    "                    np.where(df_pop_evo_popcell['FID'] == FID_popcell)[0], str(wday)\n",
    "                ].values[0]  # Ensure single value extraction\n",
    "                df_pop_evo_popcell.loc[\n",
    "                    np.where(df_pop_evo_popcell['FID'] == FID_popcell)[0], str(date)\n",
    "                ] = n_baseline * per_change / 100 + n_baseline\n",
    "\n",
    "    # Save the updated population evolution dataframe to CSV\n",
    "    df_pop_evo_popcell.to_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_popcell.csv')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a616df14",
   "metadata": {},
   "source": [
    "# Aggregate crisis population data from population cells to movement cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b577b80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw == True:\n",
    "    # Load previously saved crisis population data for popcells, dropping the unnamed index column\n",
    "    df_pop_evo_popcell = pd.read_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_popcell_raw.csv')\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "    \n",
    "else:\n",
    "    df_pop_evo_popcell = pd.read_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_popcell.csv')\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Load lookup GeoPackage linking movement grid cells to population grid cells\n",
    "gdf_lookup = gpd.read_file(\n",
    "    os.path.join(wd, 'data', 'inputs', 'grids', f'Grid_{country}_lookup_mov_to_pop.gpkg')\n",
    ")\n",
    "\n",
    "# Initialise DataFrame to store crisis population data aggregated by movement grid cells\n",
    "df_pop_evo_movcell = pd.DataFrame(\n",
    "    {column: [np.nan] * len(movcell_baseline_imput_pop) for column in columns}\n",
    ")\n",
    "\n",
    "# Insert 'FID' column at the start with indices from the baseline movement grid\n",
    "df_pop_evo_movcell.insert(\n",
    "    loc=df_pop_evo_movcell.columns.get_loc(df_pop_evo_movcell.columns[0]),\n",
    "    column='FID',\n",
    "    value=movcell_baseline_imput_pop.index\n",
    ")\n",
    "\n",
    "# Loop through each movement grid cell by index\n",
    "for i in range(len(df_pop_evo_movcell)):\n",
    "\n",
    "    # Print progress every 500 rows to monitor progress\n",
    "    if i % 500 == 0:\n",
    "        print(i / len(df_pop_evo_movcell) * 100)\n",
    "\n",
    "    # Get all population cell FIDs linked to the current movement cell\n",
    "    FIDs_pop = np.array(gdf_lookup[gdf_lookup['FID_mov'] == i]['FID_pop'])\n",
    "\n",
    "    # For each date column (skip 'FID' column)\n",
    "    for j in range(1, len(df_pop_evo_movcell.columns)):\n",
    "\n",
    "        # Extract population data from popcells for the current date and linked popcell FIDs\n",
    "        pops = np.array(df_pop_evo_popcell.iloc[FIDs_pop][df_pop_evo_popcell.columns[j]])\n",
    "\n",
    "        # Remove NaN values before aggregation\n",
    "        pops = pops[~np.isnan(pops)]\n",
    "\n",
    "        # If any valid data exists, sum it; otherwise assign NaN\n",
    "        if len(pops) > 0:\n",
    "            df_pop_evo_movcell.loc[i, df_pop_evo_movcell.columns[j]] = np.sum(pops)\n",
    "        else:\n",
    "            df_pop_evo_movcell.loc[i, df_pop_evo_movcell.columns[j]] = np.nan    \n",
    "\n",
    "# Save the aggregated movement cell population data to CSV for further analysis\n",
    "if raw == True:\n",
    "    df_pop_evo_movcell.to_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_movcell_raw.csv')\n",
    "    )\n",
    "else:\n",
    "    df_pop_evo_movcell.to_csv(\n",
    "        os.path.join(wd, 'data', 'outputs', country_short, 'evo', 'pop_evo_movcell.csv')\n",
    "    )    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
