{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda5cbc2",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2c1d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import gamma, loggamma, factorial\n",
    "import scipy.stats\n",
    "from scipy.interpolate import interp1d\n",
    "from matplotlib.ticker import AutoMinorLocator \n",
    "from matplotlib import rc, font_manager\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib import colors as mcolors\n",
    "from mycolorpy import colorlist as mcp\n",
    "from matplotlib import legend_handler\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable, axes_size\n",
    "import collections\n",
    "import matplotlib\n",
    "import shapely.geometry\n",
    "from shapely.geometry import Point\n",
    "import shapely.ops \n",
    "from geopy.geocoders import Nominatim\n",
    "import geopandas as gpd\n",
    "import mapclassify\n",
    "import jenkspy\n",
    "import contextily as cx\n",
    "import os\n",
    "import rioxarray as rx\n",
    "import pyreadr\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "from pysal.lib import weights\n",
    "from libpysal.io import open as psopen\n",
    "# import plots\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import jenkspy\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fc584e",
   "metadata": {},
   "source": [
    "# Define country and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a64f0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target country\n",
    "country = 'Colombia'\n",
    "\n",
    "# Set country-specific parameters: ISO codes and buffer size (in meters)\n",
    "if country == 'Argentina':\n",
    "    country_short = 'ARG'   # ISO 3-letter code\n",
    "    country_code = 'AR'     # ISO 2-letter code\n",
    "elif country == 'Chile':\n",
    "    country_short = 'CHL'\n",
    "    country_code = 'CL'\n",
    "elif country == 'Colombia':\n",
    "    country_short = 'COL'\n",
    "    country_code = 'CO'\n",
    "# Uncomment the following if Mexico is to be included in the analysis\n",
    "# elif country == 'Mexico':\n",
    "#     country_short = 'MEX'\n",
    "#     country_code = 'MX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c3c1f2",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b73faa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory path\n",
    "wd = (\n",
    "    '/Users/carmen/Library/CloudStorage/OneDrive-TheUniversityofLiverpool/'\n",
    "    'research/recast/latin-mobility-covid-local-files'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe12be6",
   "metadata": {},
   "source": [
    "# Define data processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c62c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flows(df_mov_evo, flow_type):\n",
    "    if flow_type == 'outflows':\n",
    "        df_flows = pd.DataFrame({'O': np.unique(df_mov_evo['O'])})\n",
    "    elif flow_type == 'inflows':\n",
    "        df_flows = pd.DataFrame({'D': np.unique(df_mov_evo['D'])})\n",
    "\n",
    "    df_flows_add = pd.DataFrame({\n",
    "        column: [np.nan for _ in range(len(df_flows))] \n",
    "        for column in df_mov_evo.columns[2:]\n",
    "    })\n",
    "\n",
    "    df_flows = pd.concat([df_flows, df_flows_add], axis=1)\n",
    "\n",
    "    for i in range(len(df_flows)):\n",
    "        if flow_type == 'outflows':\n",
    "            ID = df_flows.loc[i, 'O']\n",
    "            df = df_mov_evo[df_mov_evo['O'] == ID]\n",
    "        elif flow_type == 'inflows':\n",
    "            ID = df_flows.loc[i, 'D']\n",
    "            df = df_mov_evo[df_mov_evo['D'] == ID]\n",
    "\n",
    "        for column in df_flows.columns[1:]:\n",
    "            to_sum = [x for x in df[column] if pd.notna(x)]\n",
    "            if len(to_sum) == 0:\n",
    "                df_flows.loc[i, column] = np.nan\n",
    "            else:\n",
    "                df_flows.loc[i, column] = np.sum(to_sum)\n",
    "\n",
    "    return df_flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311a1442",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df_ts(df_flows, df_flows_baseline, initial_col):\n",
    "    evo_movs = []\n",
    "    evo_movs_baseline = []\n",
    "\n",
    "    for column in df_flows.columns[initial_col:]:\n",
    "        sum_evo_movs = []\n",
    "        sum_evo_movs_baseline = []\n",
    "\n",
    "        for i in range(len(df_flows)):\n",
    "            if not pd.isna(df_flows.loc[i, column]) and not pd.isna(df_flows_baseline.loc[i, column]):\n",
    "                sum_evo_movs.append(df_flows.loc[i, column])\n",
    "                sum_evo_movs_baseline.append(df_flows_baseline.loc[i, column])\n",
    "\n",
    "        if len(sum_evo_movs) > 0:\n",
    "            evo_movs.append(np.sum(sum_evo_movs))\n",
    "            evo_movs_baseline.append(np.sum(sum_evo_movs_baseline))\n",
    "        else:\n",
    "            evo_movs.append(np.nan)\n",
    "            evo_movs_baseline.append(np.nan)\n",
    "\n",
    "    df_ts = pd.DataFrame({\n",
    "        'date': df_flows.columns[initial_col:], \n",
    "        'movements': evo_movs, \n",
    "        'baseline': evo_movs_baseline\n",
    "    })\n",
    "\n",
    "    # Replace zeros and infs by NaNs, then replace NaNs by average of closest 15 observations in time series\n",
    "    for i in range(len(df_ts)):\n",
    "        if df_ts.loc[i, 'movements'] == 0:\n",
    "            df_ts.loc[i, 'movements'] = np.nan\n",
    "\n",
    "    df_ts.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df_ts['isna'] = df_ts['movements'].isna()\n",
    "    df_ts['movements_fill'] = df_ts['movements']\n",
    "    df_ts_dropna = df_ts.dropna()\n",
    "\n",
    "    indexes = df_ts[df_ts['isna']].index\n",
    "    for target_index in indexes:\n",
    "        # Calculate the absolute difference between the row indices and the target index\n",
    "        df_ts_dropna.loc[:, 'diff'] = abs(df_ts_dropna.index - target_index)\n",
    "        # Sort the DataFrame by this difference\n",
    "        df_sorted = df_ts_dropna.sort_values(by='diff')\n",
    "        # Select the top 15 rows\n",
    "        nearest_rows = df_sorted.head(15)\n",
    "        # Replace NaN by mean of nearest rows\n",
    "        df_ts.loc[target_index, 'movements_fill'] = np.mean(nearest_rows['movements'])\n",
    "        df_ts_dropna.drop(columns=['diff'], inplace=True)\n",
    "\n",
    "    df_ts['rolling'] = df_ts['movements_fill'].rolling(window=15).mean()\n",
    "\n",
    "    # Repeat for baseline\n",
    "    for i in range(len(df_ts)):\n",
    "        if df_ts.loc[i, 'baseline'] == 0:\n",
    "            df_ts.loc[i, 'baseline'] = np.nan\n",
    "\n",
    "    df_ts['isna'] = df_ts['baseline'].isna()\n",
    "    df_ts['baseline_fill'] = df_ts['baseline']\n",
    "    df_ts_dropna = df_ts.dropna()\n",
    "\n",
    "    indexes = df_ts[df_ts['isna']].index\n",
    "    for target_index in indexes:\n",
    "        df_ts_dropna.loc[:, 'diff'] = abs(df_ts_dropna.index - target_index)\n",
    "        df_sorted = df_ts_dropna.sort_values(by='diff')\n",
    "        nearest_rows = df_sorted.head(15)\n",
    "        df_ts.loc[target_index, 'baseline_fill'] = np.mean(nearest_rows['baseline'])\n",
    "        df_ts_dropna.drop(columns=['diff'], inplace=True)\n",
    "\n",
    "    df_ts['rolling_baseline'] = df_ts['baseline_fill'].rolling(window=15).mean()\n",
    "\n",
    "    df_ts['perchange'] = [\n",
    "        (df_ts.loc[i, 'movements_fill'] - df_ts.loc[i, 'baseline_fill']) / df_ts.loc[i, 'baseline_fill'] * 100 \n",
    "        for i in range(len(df_ts))\n",
    "    ]\n",
    "    df_ts['rolling_perchange'] = df_ts['perchange'].rolling(window=30).mean()\n",
    "\n",
    "    return df_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e01eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df_ts_weekly(df_ts):\n",
    "    weeks = int(len(df_ts) / 7)\n",
    "    \n",
    "    df_ts_weekly = pd.DataFrame({'week_no': range(weeks)})\n",
    "    df_ts_weekly['week_start'] = [df_ts.loc[i*7, 'date'] for i in range(weeks)]\n",
    "    df_ts_weekly['movements'] = [np.sum(df_ts.loc[i*7:(i+1)*7-1, 'movements']) for i in range(weeks)]\n",
    "    df_ts_weekly['baseline'] = [np.sum(df_ts.loc[i*7:(i+1)*7-1, 'baseline']) for i in range(weeks)]\n",
    "    df_ts_weekly['movements_fill'] = [np.sum(df_ts.loc[i*7:(i+1)*7-1, 'movements_fill']) for i in range(weeks)]\n",
    "    df_ts_weekly['baseline_fill'] = [np.sum(df_ts.loc[i*7:(i+1)*7-1, 'baseline_fill']) for i in range(weeks)]\n",
    "    df_ts_weekly['perchange'] = [\n",
    "        (df_ts_weekly.loc[i, 'movements_fill'] - df_ts_weekly.loc[i, 'baseline_fill']) / df_ts_weekly.loc[i, 'baseline_fill'] * 100 \n",
    "        for i in range(weeks)\n",
    "    ]\n",
    "\n",
    "    return df_ts_weekly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bacbe4",
   "metadata": {},
   "source": [
    "# Load more data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd9d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stringency = pd.read_csv(wd + '/data/inputs/covid-stringency/owid-covid-data.csv')\n",
    "df_stringency = df_stringency[df_stringency['location'] == str(country).capitalize()].reset_index(drop=True)\n",
    "\n",
    "baseline_pop_imput = gpd.read_file(\n",
    "    wd + '/data/outputs/' + country_short + '/grids-with-data/movcell-baseline-imput-pop-with-exo-var/movcell-baseline-imput-pop-with-exo-var.gpkg'\n",
    ")\n",
    "\n",
    "baseline_mov_dist = pd.read_csv(\n",
    "    wd + '/data/outputs/' + country_short + '/baseline/movcell-baseline-mov-dist-with-exo-var.csv'\n",
    ").drop(['Unnamed: 0'], axis=1)                              "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30534ac2",
   "metadata": {},
   "source": [
    "# Evolution by day or by week, distance >= 0 or > 0, data raw or processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3a8d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = True\n",
    "raw = False\n",
    "adjust = True\n",
    "\n",
    "if dist == True:\n",
    "    dist = '_dist'\n",
    "else:\n",
    "    dist = ''\n",
    "if raw == True:\n",
    "    raw = '_raw'\n",
    "    adjust = ''\n",
    "else:\n",
    "    raw = '' \n",
    "    if adjust == True:\n",
    "        adjust = '_adjust'\n",
    "    else:\n",
    "        adjust = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b181a2b",
   "metadata": {},
   "source": [
    "# Load movement data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0851bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mov_evo = pd.read_csv(\n",
    "    wd + '/data/outputs/' + country_short + '/evo/mov_evo' + dist + raw + adjust + '.csv'\n",
    ").drop('Unnamed: 0', axis=1)\n",
    "\n",
    "df_mov_evo_baseline = pd.read_csv(\n",
    "    wd + '/data/outputs/' + country_short + '/evo/mov_evo_baseline' + dist + raw + '.csv'\n",
    ").drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ac8400",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = max(min(list(df_mov_evo_baseline.columns[2:])), min(list(df_mov_evo.columns[2:])))\n",
    "end_date = min(max(list(df_mov_evo_baseline.columns[2:])), max(list(df_mov_evo.columns[2:])))\n",
    "\n",
    "start_date_index = np.where(df_mov_evo.columns==start_date)[0][0]\n",
    "start_date_baseline_index = np.where(df_mov_evo_baseline.columns==start_date)[0][0]\n",
    "\n",
    "end_date_index = np.where(df_mov_evo.columns==end_date)[0][0]\n",
    "end_date_baseline_index = np.where(df_mov_evo_baseline.columns==end_date)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d54762",
   "metadata": {},
   "source": [
    "# Create and save data for distances overtime - takes long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ada4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ONLY UNCOMMENT TO MAKE THE COMPUTATIONS AGAIN\n",
    "\n",
    "# day = True\n",
    "\n",
    "# if day == True:\n",
    "#     df_dist = df_mov_evo.iloc[:,0:2]\n",
    "#     df_dist_baseline = df_mov_evo_baseline.iloc[:,0:2]\n",
    "\n",
    "#     for j in range(start_date_index, end_date_index, 1):\n",
    "#         df_dist[df_mov_evo.columns[j]] = [np.sum(df_mov_evo.loc[i,df_mov_evo.columns[j:j+1]]) for i in range(len(df_dist))]\n",
    "\n",
    "#     for j in range(start_date_baseline_index, end_date_baseline_index, 1):\n",
    "#         df_dist_baseline[df_mov_evo_baseline.columns[j]] = [np.sum(df_mov_evo_baseline.loc[i,df_mov_evo_baseline.columns[j:j+1]]) for i in range(len(df_dist_baseline))]\n",
    "# else:\n",
    "#     df_dist = df_mov_evo.iloc[:,0:2]\n",
    "#     df_dist_baseline = df_mov_evo_baseline.iloc[:,0:2]\n",
    "\n",
    "#     for j in range(start_date_index, end_date_index, 7):\n",
    "#         df_dist[df_mov_evo.columns[j]] = [np.sum(df_mov_evo.loc[i,df_mov_evo.columns[j:j+7]]) for i in range(len(df_dist))]\n",
    "\n",
    "#     for j in range(start_date_baseline_index, end_date_baseline_index, 7):\n",
    "#         df_dist_baseline[df_mov_evo_baseline.columns[j]] = [np.sum(df_mov_evo_baseline.loc[i,df_mov_evo_baseline.columns[j:j+7]]) for i in range(len(df_dist_baseline))]\n",
    "    \n",
    "# distances = []\n",
    "# for i in range(len(df_dist)):\n",
    "#     O = df_dist.loc[i, 'O']\n",
    "#     D = df_dist.loc[i, 'D']\n",
    "# #     print(baseline_mov_dist[(baseline_mov_dist['O']==O) & (baseline_mov_dist['D']==D) & (baseline_mov_dist['wday']==0)].reset_index(drop=True).loc[0,'dist'])\n",
    "#     distances.append(baseline_mov_dist[(baseline_mov_dist['O']==O) & (baseline_mov_dist['D']==D) & (baseline_mov_dist['wday']==0)].reset_index(drop=True).loc[0,'dist'])\n",
    "# df_dist.insert(2, 'dist', distances)\n",
    "# df_dist_baseline.insert(2, 'dist', distances)\n",
    "\n",
    "\n",
    "# if day==True:\n",
    "#     df_dist.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_day' + dist + raw + adjust + '.csv')\n",
    "#     df_dist_baseline.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_day_baseline' + dist + raw + adjust + '.csv')\n",
    "# else:\n",
    "#     df_dist.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_week' + dist + raw + adjust + '.csv')\n",
    "#     df_dist_baseline.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_week_baseline' + dist + raw + adjust + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fe0170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ONLY UNCOMMENT TO MAKE THE COMPUTATIONS AGAIN\n",
    "\n",
    "# day = False\n",
    "\n",
    "# if day == True:\n",
    "#     df_dist = df_mov_evo.iloc[:,0:2]\n",
    "#     df_dist_baseline = df_mov_evo_baseline.iloc[:,0:2]\n",
    "\n",
    "#     for j in range(start_date_index, end_date_index, 1):\n",
    "#         df_dist[df_mov_evo.columns[j]] = [np.sum(df_mov_evo.loc[i,df_mov_evo.columns[j:j+1]]) for i in range(len(df_dist))]\n",
    "\n",
    "#     for j in range(start_date_baseline_index, end_date_baseline_index, 1):\n",
    "#         df_dist_baseline[df_mov_evo_baseline.columns[j]] = [np.sum(df_mov_evo_baseline.loc[i,df_mov_evo_baseline.columns[j:j+1]]) for i in range(len(df_dist_baseline))]\n",
    "# else:\n",
    "#     df_dist = df_mov_evo.iloc[:,0:2]\n",
    "#     df_dist_baseline = df_mov_evo_baseline.iloc[:,0:2]\n",
    "\n",
    "#     for j in range(start_date_index, end_date_index, 7):\n",
    "#         df_dist[df_mov_evo.columns[j]] = [np.sum(df_mov_evo.loc[i,df_mov_evo.columns[j:j+7]]) for i in range(len(df_dist))]\n",
    "\n",
    "#     for j in range(start_date_baseline_index, end_date_baseline_index, 7):\n",
    "#         df_dist_baseline[df_mov_evo_baseline.columns[j]] = [np.sum(df_mov_evo_baseline.loc[i,df_mov_evo_baseline.columns[j:j+7]]) for i in range(len(df_dist_baseline))]\n",
    "    \n",
    "# distances = []\n",
    "# for i in range(len(df_dist)):\n",
    "#     O = df_dist.loc[i, 'O']\n",
    "#     D = df_dist.loc[i, 'D']\n",
    "# #     print(baseline_mov_dist[(baseline_mov_dist['O']==O) & (baseline_mov_dist['D']==D) & (baseline_mov_dist['wday']==0)].reset_index(drop=True).loc[0,'dist'])\n",
    "#     distances.append(baseline_mov_dist[(baseline_mov_dist['O']==O) & (baseline_mov_dist['D']==D) & (baseline_mov_dist['wday']==0)].reset_index(drop=True).loc[0,'dist'])\n",
    "# df_dist.insert(2, 'dist', distances)\n",
    "# df_dist_baseline.insert(2, 'dist', distances)\n",
    "\n",
    "\n",
    "# if day==True:\n",
    "#     df_dist.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_day' + dist + raw + adjust + '.csv')\n",
    "#     df_dist_baseline.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_day_baseline' + dist + raw + adjust + '.csv')\n",
    "# else:\n",
    "#     df_dist.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_week' + dist + raw + adjust + '.csv')\n",
    "#     df_dist_baseline.to_csv(wd + '/data/outputs/' + country_short + '/evo/evo_distance_week_baseline' + dist + raw + adjust + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ae2516",
   "metadata": {},
   "source": [
    "# Load data for distances over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb052bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "day = False\n",
    "\n",
    "if day:\n",
    "    df_dist = pd.read_csv(\n",
    "        wd + f'/data/outputs/{country_short}/evo/evo_distance_day{dist}{raw}{adjust}.csv'\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "    df_dist_baseline = pd.read_csv(\n",
    "        wd + f'/data/outputs/{country_short}/evo/evo_distance_day_baseline{dist}{raw}{adjust}.csv'\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "else:\n",
    "    df_dist = pd.read_csv(\n",
    "        wd + f'/data/outputs/{country_short}/evo/evo_distance_week{dist}{raw}{adjust}.csv'\n",
    "    ).drop('Unnamed: 0', axis=1)\n",
    "    df_dist_baseline = pd.read_csv(\n",
    "        wd + f'/data/outputs/{country_short}/evo/evo_distance_week_baseline{dist}{raw}{adjust}.csv'\n",
    "    ).drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_quantile(values, quantiles, sample_weight):\n",
    "    \"\"\"\n",
    "    Compute weighted quantiles for the given values, sample weights, and quantiles.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    values : array-like\n",
    "        The data values.\n",
    "    quantiles : array-like\n",
    "        Quantiles to compute, e.g., [0.25, 0.5, 0.75].\n",
    "    sample_weight : array-like\n",
    "        Weights for each data point.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    numpy.ndarray\n",
    "        Weighted quantile values corresponding to the requested quantiles.\n",
    "    \"\"\"\n",
    "    values = np.array(values)\n",
    "    sample_weight = np.array(sample_weight)\n",
    "    \n",
    "    sorter = np.argsort(values)\n",
    "    values = values[sorter]\n",
    "    sample_weight = sample_weight[sorter]\n",
    "    \n",
    "    cumulative_weights = np.cumsum(sample_weight)\n",
    "    cumulative_weights /= cumulative_weights[-1]  # Normalize to [0,1]\n",
    "    \n",
    "    return np.interp(quantiles, cumulative_weights, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eac18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distribution(df_dist):\n",
    "    \"\"\"\n",
    "    Compute weighted percentiles (5th, 25th, 50th, 75th, 95th) \n",
    "    of the 'dist' column for each distribution column in df_dist.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_dist : pandas.DataFrame\n",
    "        DataFrame where the first 3 columns are metadata, and subsequent columns\n",
    "        contain weights corresponding to distances in the 'dist' column.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with columns ['date', 'p005', 'p025', 'p050', 'p075', 'p095']\n",
    "        representing weighted percentiles per distribution column.\n",
    "    \"\"\"\n",
    "    df_distr = pd.DataFrame({'date': [], 'p005': [], 'p025': [], 'p050': [], 'p075': [], 'p095': []})\n",
    "\n",
    "    for column in df_dist.columns[3:]:\n",
    "        row = [column]\n",
    "        weights = df_dist[column] / np.sum(df_dist[column])\n",
    "        percentiles = weighted_quantile(df_dist['dist'], [0.05, 0.25, 0.5, 0.75, 0.95], weights)\n",
    "        row.extend(percentiles)\n",
    "        df_distr.loc[len(df_distr)] = row\n",
    "    \n",
    "    return df_distr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db405764",
   "metadata": {},
   "source": [
    "# By density class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db738eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_density = 5\n",
    "\n",
    "# Calculate Jenks natural breaks on 'density' (dropping NaNs)\n",
    "breaks_density = jenkspy.jenks_breaks(baseline_pop_imput.dropna(subset=['density'])['density'], n_classes=n_class_density)\n",
    "breaks_density[0] = breaks_density[0] - 10**(-10)  # Adjust lower bound slightly to include minimum value\n",
    "\n",
    "# Assign density classes using pd.cut based on Jenks breaks\n",
    "baseline_pop_imput['class_density'] = pd.cut(\n",
    "    baseline_pop_imput['density'], \n",
    "    bins=breaks_density, \n",
    "    labels=[i for i in range(n_class_density)]\n",
    ")\n",
    "baseline_pop_imput['class_density'] = pd.to_numeric(baseline_pop_imput['class_density'])\n",
    "\n",
    "n_percentiles = 5\n",
    "df_distr_class_density = np.zeros((n_class_density, n_percentiles, len(df_dist.columns[3:])))\n",
    "df_distr_class_density_baseline = np.zeros((n_class_density, n_percentiles, len(df_dist_baseline.columns[3:])))\n",
    "\n",
    "class_density = np.unique(baseline_pop_imput['class_density'])\n",
    "n_class_density = len(class_density[~np.isnan(class_density)])\n",
    "\n",
    "for i in range(n_class_density):\n",
    "    # Select indexes for current density class\n",
    "    indexes = set(baseline_pop_imput[baseline_pop_imput['class_density'] == i].index)\n",
    "    \n",
    "    # Mask for df_dist: origins in current class and distance >= 100000\n",
    "    mask = (df_dist['O'].isin(indexes)) & (df_dist['dist'] >= 100000)\n",
    "    df_dist_class_density = df_dist[mask].reset_index(drop=True)\n",
    "    \n",
    "    # Mask for baseline distances (distance >= 100000)\n",
    "    mask_baseline = df_dist_baseline['dist'] >= 100000\n",
    "    df_dist_class_density_baseline = df_dist_baseline[mask_baseline].reset_index(drop=True)\n",
    "    \n",
    "    # Compute weighted distribution for current class\n",
    "    df_distr = compute_distribution(df_dist_class_density)\n",
    "    for j in range(1, len(df_distr.columns)):\n",
    "        df_distr_class_density[i][j-1][:] = df_distr[df_distr.columns[j]]\n",
    "        \n",
    "    df_distr_baseline = compute_distribution(df_dist_class_density_baseline)\n",
    "    for j in range(1, len(df_distr_baseline.columns)):\n",
    "        df_distr_class_density_baseline[i][j-1][:] = df_distr_baseline[df_distr_baseline.columns[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abba470",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    1, n_class_density, sharey=True,\n",
    "    gridspec_kw={'hspace': 0.05, 'wspace': 0.05},\n",
    "    figsize=(37.5, 7.5)\n",
    ")\n",
    "\n",
    "for i in range(n_class_density):\n",
    "\n",
    "    axs[i].tick_params(\n",
    "        axis='both', which='both', width=0, length=0,\n",
    "        color='k', labelsize=20, pad=9\n",
    "    )\n",
    "\n",
    "    df_distr_class_density_plot = pd.DataFrame({\n",
    "        'lower': df_distr_class_density[i][1],\n",
    "        'median': df_distr_class_density[i][2],\n",
    "        'upper': df_distr_class_density[i][3]\n",
    "    })\n",
    "    for column in df_distr_class_density_plot.columns:\n",
    "        df_distr_class_density_plot['rolling_' + str(column)] = df_distr_class_density_plot[column].ewm(span=12).mean()\n",
    "        df_distr_class_density_plot['rolling_' + str(column)] = scipy.signal.savgol_filter(\n",
    "            df_distr_class_density_plot['rolling_' + str(column)],\n",
    "            window_length=12, polyorder=3\n",
    "        )\n",
    "    # axs[i].plot(\n",
    "    #     np.arange(len(df_distr_class_density_plot['rolling_median'])) * 7,\n",
    "    #     df_distr_class_density_plot['rolling_median'],\n",
    "    #     color='k', lw=3.5, zorder=6\n",
    "    # )\n",
    "    # axs[i].fill_between(\n",
    "    #     np.arange(len(df_distr_class_density_plot['rolling_median'])) * 7,\n",
    "    #     df_distr_class_density_plot['rolling_lower'],\n",
    "    #     df_distr_class_density_plot['rolling_upper'],\n",
    "    #     color='k', alpha=0.2, lw=0, zorder=6\n",
    "    # )\n",
    "\n",
    "    df_distr_class_density_baseline_plot = pd.DataFrame({\n",
    "        'lower': df_distr_class_density_baseline[i][1],\n",
    "        'median': df_distr_class_density_baseline[i][2],\n",
    "        'upper': df_distr_class_density_baseline[i][3]\n",
    "    })\n",
    "    for column in df_distr_class_density_baseline_plot.columns:\n",
    "        df_distr_class_density_baseline_plot['rolling_' + str(column)] = df_distr_class_density_baseline_plot[column].ewm(span=12).mean()\n",
    "        df_distr_class_density_baseline_plot['rolling_' + str(column)] = scipy.signal.savgol_filter(\n",
    "            df_distr_class_density_baseline_plot['rolling_' + str(column)],\n",
    "            window_length=12, polyorder=3\n",
    "        )\n",
    "    # axs[i].plot(\n",
    "    #     np.arange(len(df_distr_class_density_baseline_plot['rolling_median'])) * 7,\n",
    "    #     df_distr_class_density_baseline_plot['rolling_median'],\n",
    "    #     color='b', linestyle=':', lw=3.5, zorder=7\n",
    "    # )\n",
    "    # axs[i].fill_between(\n",
    "    #     np.arange(len(df_distr_class_density_baseline_plot['rolling_median'])) * 7,\n",
    "    #     df_distr_class_density_baseline_plot['rolling_lower'],\n",
    "    #     df_distr_class_density_baseline_plot['rolling_upper'],\n",
    "    #     color='b', alpha=0.2, lw=3.5, zorder=6\n",
    "    # )\n",
    "\n",
    "    perchange = [\n",
    "        (df_distr_class_density_plot.loc[j, 'rolling_median'] -\n",
    "         df_distr_class_density_baseline_plot.loc[j, 'rolling_median']) /\n",
    "        df_distr_class_density_baseline_plot.loc[j, 'rolling_median'] * 100\n",
    "        for j in range(len(df_distr_class_density_baseline_plot['rolling_median']))\n",
    "    ]\n",
    "\n",
    "    axs[i].plot(\n",
    "        np.arange(len(df_distr_class_density_baseline_plot['rolling_median'])) * 7,\n",
    "        perchange, color='k', lw=3.5, zorder=6\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        np.arange(len(df_distr_class_density_baseline_plot['rolling_median'])) * 7,\n",
    "        np.zeros(len(df_distr_class_density_baseline_plot['rolling_median'])),\n",
    "        color='k', linestyle=':', lw=3.5, zorder=6\n",
    "    )\n",
    "\n",
    "    stringencies = []\n",
    "    for date in df_mov_evo.columns[2:]:\n",
    "        stringencies.append(df_stringency[df_stringency['date'] == date].reset_index(drop=True).loc[0, 'stringency_index'])\n",
    "\n",
    "    # ymin = int(min([-100, np.min(df_distr_class_density[i][1])]))\n",
    "    # try:\n",
    "    #     ymax = int(max([101, np.max([i for i in df_ts_weekly_class_density.flatten() if i < np.max(df_ts_weekly_class_density)]) + 1]))\n",
    "    # except:\n",
    "    #     ymax = 101\n",
    "\n",
    "    ymin = -75\n",
    "    ymax = 75\n",
    "    for l in range(len(stringencies)):\n",
    "        try:\n",
    "            rgba = matplotlib.cm.gist_heat(1 - (stringencies[l] - min(stringencies)) / max(stringencies))\n",
    "        except:\n",
    "            rgba = matplotlib.cm.gist_heat(1 - (stringencies[l - 1] - min(stringencies)) / max(stringencies))\n",
    "        x = [l - 0.50, l + 0.50]\n",
    "        axs[i].fill_between(x, ymin, ymax, color=rgba, alpha=0.8, edgecolor='None', linewidth=0, zorder=0)\n",
    "\n",
    "    xticks = []\n",
    "    xticks_labels = ['Apr 2020', 'Oct 2020', 'Apr 2021', 'Oct 2021', 'April 2022']\n",
    "    for l in range(0, len(df_mov_evo.columns[2:])):\n",
    "        if l % 183 == 0:\n",
    "            xticks.append(l)\n",
    "    axs[i].set_xticks(xticks, xticks_labels)\n",
    "    axs[i].tick_params(axis='x', bottom=True, labelsize=28, pad=12, rotation=90)\n",
    "\n",
    "    # yticks = []\n",
    "    # for l in range(ymin, ymax):\n",
    "    #     if (l + 50000) % 150000 == 0:\n",
    "    #         yticks.append(l)\n",
    "    # axs[i].set_yticks(yticks, [int(y / 1000) for y in yticks])\n",
    "    # for y in yticks:\n",
    "    #     axs[i].plot([0, len(df_mov_evo.columns[2:])], [y, y], color='gray', lw=0.7, zorder=0)\n",
    "    # axs[i].tick_params(axis='y', labelsize=28, pad=12, rotation=0)\n",
    "\n",
    "plt.savefig(\n",
    "    wd + '/plots/evolution/distance/by-density/' + country_short +\n",
    "    '/evo_distance_perchange' + dist + raw + adjust + '_by_origin.pdf',\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9afb69",
   "metadata": {},
   "source": [
    "# By rdi class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class_rdi = 3\n",
    "\n",
    "breaks_rdi = jenkspy.jenks_breaks(\n",
    "    baseline_pop_imput.dropna(subset=['rdi'])['rdi'],\n",
    "    n_classes=n_class_rdi\n",
    ")\n",
    "breaks_rdi[0] = breaks_rdi[0] - 10**(-10)\n",
    "\n",
    "# baseline_pop_imput['class_rdi'] = pd.qcut(baseline_pop_imput['rdi'], q=n_class_rdi, labels=[i for i in range(n_class_rdi)])\n",
    "baseline_pop_imput['class_rdi'] = pd.cut(\n",
    "    baseline_pop_imput['rdi'],\n",
    "    bins=breaks_rdi,\n",
    "    labels=[i for i in range(n_class_rdi)]\n",
    ")\n",
    "baseline_pop_imput['class_rdi'] = pd.to_numeric(baseline_pop_imput['class_rdi'])\n",
    "\n",
    "n_percentiles = 5\n",
    "df_distr_class_rdi = np.zeros((n_class_rdi, n_percentiles, len(df_dist.columns[3:])))\n",
    "df_distr_class_rdi_baseline = np.zeros((n_class_rdi, n_percentiles, len(df_dist_baseline.columns[3:])))\n",
    "\n",
    "class_rdi = np.unique(baseline_pop_imput['class_rdi'])\n",
    "n_class_rdi = len(class_rdi[~np.isnan(class_rdi)])\n",
    "\n",
    "for i in range(n_class_rdi):\n",
    "\n",
    "    indexes = set(baseline_pop_imput[baseline_pop_imput['class_rdi'] == i].index)\n",
    "    mask = (df_dist['O'].isin(indexes)) & (df_dist['dist'] >= 100000)\n",
    "    df_dist_class_rdi = df_dist[mask].reset_index(drop=True)\n",
    "    \n",
    "    mask_baseline = df_dist['dist'] >= 100000\n",
    "    df_dist_class_rdi_baseline = df_dist_baseline[mask_baseline].reset_index(drop=True)\n",
    "    \n",
    "    df_distr = compute_distribution(df_dist_class_rdi)\n",
    "    for j in range(1, len(df_distr.columns)):\n",
    "        df_distr_class_rdi[i][j-1][:] = df_distr[df_distr.columns[j]]\n",
    "        \n",
    "    df_distr_baseline = compute_distribution(df_dist_class_rdi_baseline)\n",
    "    for j in range(1, len(df_distr_baseline.columns)):\n",
    "        df_distr_class_rdi_baseline[i][j-1][:] = df_distr_baseline[df_distr_baseline.columns[j]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745e96dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(\n",
    "    1, n_class_rdi, sharey=True,\n",
    "    gridspec_kw={'hspace': 0.05, 'wspace': 0.05},\n",
    "    figsize=(37.5, 7.5)\n",
    ")\n",
    "\n",
    "for i in range(n_class_rdi):\n",
    "    \n",
    "    axs[i].tick_params(\n",
    "        axis='both', which='both', width=0, length=0,\n",
    "        color='k', labelsize=20, pad=9\n",
    "    )\n",
    "\n",
    "    df_distr_class_rdi_plot = pd.DataFrame({\n",
    "        'lower': df_distr_class_rdi[i][1],\n",
    "        'median': df_distr_class_rdi[i][2],\n",
    "        'upper': df_distr_class_rdi[i][3]\n",
    "    })\n",
    "    for column in df_distr_class_rdi_plot.columns:\n",
    "        df_distr_class_rdi_plot['rolling_' + str(column)] = df_distr_class_rdi_plot[column].ewm(span=12).mean()\n",
    "        df_distr_class_rdi_plot['rolling_' + str(column)] = scipy.signal.savgol_filter(\n",
    "            df_distr_class_rdi_plot['rolling_' + str(column)],\n",
    "            window_length=12, polyorder=3\n",
    "        )\n",
    "    # axs[i].plot(np.arange(len(df_distr_class_rdi_plot['rolling_median']))*7,\n",
    "    #             df_distr_class_rdi_plot['rolling_median'],\n",
    "    #             color='k', lw=3.5, zorder=6)\n",
    "    # axs[i].fill_between(np.arange(len(df_distr_class_rdi_plot['rolling_median']))*7,\n",
    "    #                     df_distr_class_rdi_plot['rolling_lower'],\n",
    "    #                     df_distr_class_rdi_plot['rolling_upper'],\n",
    "    #                     color='k', alpha=0.2, lw=0, zorder=6)\n",
    "\n",
    "    df_distr_class_rdi_baseline_plot = pd.DataFrame({\n",
    "        'lower': df_distr_class_rdi_baseline[i][1],\n",
    "        'median': df_distr_class_rdi_baseline[i][2],\n",
    "        'upper': df_distr_class_rdi_baseline[i][3]\n",
    "    })\n",
    "    for column in df_distr_class_rdi_baseline_plot.columns:\n",
    "        df_distr_class_rdi_baseline_plot['rolling_' + str(column)] = df_distr_class_rdi_baseline_plot[column].ewm(span=12).mean()\n",
    "        df_distr_class_rdi_baseline_plot['rolling_' + str(column)] = scipy.signal.savgol_filter(\n",
    "            df_distr_class_rdi_baseline_plot['rolling_' + str(column)],\n",
    "            window_length=12, polyorder=3\n",
    "        )\n",
    "    # axs[i].plot(np.arange(len(df_distr_class_rdi_baseline_plot['rolling_median']))*7,\n",
    "    #             df_distr_class_rdi_baseline_plot['rolling_median'],\n",
    "    #             color='b', linestyle=':', lw=3.5, zorder=7)\n",
    "    # axs[i].fill_between(np.arange(len(df_distr_class_rdi_baseline_plot['rolling_median']))*7,\n",
    "    #                     df_distr_class_rdi_baseline_plot['rolling_lower'],\n",
    "    #                     df_distr_class_rdi_baseline_plot['rolling_upper'],\n",
    "    #                     color='b', alpha=0.2, lw=2.5, zorder=6)\n",
    "    \n",
    "    perchange = [\n",
    "        (df_distr_class_rdi_plot.loc[j, 'rolling_median'] -\n",
    "         df_distr_class_rdi_baseline_plot.loc[j, 'rolling_median']) / \n",
    "        df_distr_class_rdi_baseline_plot.loc[j, 'rolling_median'] * 100\n",
    "        for j in range(len(df_distr_class_rdi_baseline_plot['rolling_median']))\n",
    "    ]\n",
    "    \n",
    "    axs[i].plot(\n",
    "        np.arange(len(df_distr_class_rdi_baseline_plot['rolling_median']))*7,\n",
    "        perchange,\n",
    "        color='k', lw=3.5, zorder=6\n",
    "    )\n",
    "    axs[i].plot(\n",
    "        np.arange(len(df_distr_class_rdi_baseline_plot['rolling_median']))*7,\n",
    "        np.zeros(len(df_distr_class_rdi_baseline_plot['rolling_median'])),\n",
    "        color='k', linestyle=':', lw=3.5, zorder=6\n",
    "    )\n",
    "\n",
    "    stringencies = []\n",
    "    for date in df_mov_evo.columns[2:]:\n",
    "        stringencies.append(df_stringency[df_stringency['date'] == date].reset_index(drop=True).loc[0, 'stringency_index'])\n",
    "\n",
    "    # ymin = int(min([-100, np.min(df_distr_class_rdi[i][1])]))\n",
    "    # try:\n",
    "    #     ymax = int(max([101, np.max([i for i in df_ts_weekly_class_rdi.flatten() if i < np.max(df_ts_weekly_class_rdi)])+1]))\n",
    "    # except:\n",
    "    #     ymax = 101\n",
    "    \n",
    "    ymin = -45\n",
    "    ymax = 45\n",
    "    for l in range(len(stringencies)):\n",
    "        try:\n",
    "            rgba = matplotlib.cm.gist_heat(\n",
    "                1 - (stringencies[l] - min(stringencies)) / max(stringencies)\n",
    "            )\n",
    "        except:\n",
    "            rgba = matplotlib.cm.gist_heat(\n",
    "                1 - (stringencies[l-1] - min(stringencies)) / max(stringencies)\n",
    "            )\n",
    "        x = [l - 0.50, l + 0.50]\n",
    "        axs[i].fill_between(x, ymin, ymax, color=rgba, alpha=0.8, edgecolor='None', linewidth=0, zorder=0)\n",
    "\n",
    "    xticks = []\n",
    "    xticks_labels = ['Apr 2020', 'Oct 2020', 'Apr 2021', 'Oct 2021', 'April 2022']\n",
    "    for l in range(0, len(df_mov_evo.columns[2:])):\n",
    "        if l % 183 == 0:\n",
    "            xticks.append(l)\n",
    "    axs[i].set_xticks(xticks, xticks_labels)\n",
    "    axs[i].tick_params(axis='x', bottom=True, labelsize=28, pad=12, rotation=90)\n",
    "\n",
    "    # yticks = []\n",
    "    # for l in range(ymin, ymax):\n",
    "    #     if (l) % 100000 == 0:\n",
    "    #         yticks.append(l)\n",
    "    # axs[i].set_yticks(yticks, [int(y/1000) for y in yticks])\n",
    "    # for y in yticks:\n",
    "    #     axs[i].plot([0, len(df_mov_evo.columns[2:])], [y, y], color='gray', lw=0.7, zorder=0)\n",
    "    # axs[i].tick_params(axis='y', labelsize=28, pad=12, rotation=0)\n",
    "\n",
    "plt.savefig(\n",
    "    wd + '/plots/evolution/distance/by-rdi/' + country_short +\n",
    "    '/evo_distance_perchange' + dist + raw + adjust + '_by_origin.pdf',\n",
    "    bbox_inches='tight'\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
