{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91752a69",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2763cb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae987b1",
   "metadata": {},
   "source": [
    "# Define country and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae713a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select target country\n",
    "country = 'Colombia'\n",
    "\n",
    "# Set country-specific parameters: ISO codes and buffer size (in meters)\n",
    "if country == 'Argentina':\n",
    "    country_short = 'ARG'   # ISO 3-letter code\n",
    "    country_code = 'AR'     # ISO 2-letter code\n",
    "elif country == 'Chile':\n",
    "    country_short = 'CHL'\n",
    "    country_code = 'CL'\n",
    "elif country == 'Colombia':\n",
    "    country_short = 'COL'\n",
    "    country_code = 'CO'\n",
    "# Uncomment the following if Mexico is to be included in the analysis\n",
    "# elif country == 'Mexico':\n",
    "#     country_short = 'MEX'\n",
    "#     country_code = 'MX'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba05a2b",
   "metadata": {},
   "source": [
    "# Set working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d22703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define working directory path\n",
    "wd = (\n",
    "    '/your/path/to/working/directory/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc231dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imputed population baseline data with exogenous variables from a GeoPackage file\n",
    "movcell_baseline_imput_pop = gpd.read_file(\n",
    "    wd + '/data/outputs/' + country_short + '/grids-with-data/movcell-baseline-imput-pop-with-exo-var/movcell-baseline-imput-pop-with-exo-var.gpkg'\n",
    ")\n",
    "\n",
    "# Add a new column 'FID' equal to the current index (used for merging or referencing)\n",
    "movcell_baseline_imput_pop['FID'] = movcell_baseline_imput_pop.index\n",
    "\n",
    "# Load the baseline movement CSV data\n",
    "baseline_mov = pd.read_csv(wd + '/data/outputs/' + country_short + '/baseline/baseline_mov.csv')\n",
    "\n",
    "# Replace missing values with -1 to handle missing data uniformly\n",
    "baseline_mov = baseline_mov.fillna(-1)\n",
    "baseline_mov = baseline_mov.replace(-999, -1)\n",
    "\n",
    "# The following commented code could be used to merge grid population data if needed\n",
    "# grid_pop = pd.merge(grid_pop, baseline_pop, on='FID', how='left')\n",
    "\n",
    "# Identify rows where origin and destination are the same (O == D) to exclude them\n",
    "index_to_drop = []\n",
    "for i in range(len(baseline_mov)):\n",
    "    if baseline_mov.loc[i, 'O'] == baseline_mov.loc[i, 'D']:\n",
    "        index_to_drop.append(i)\n",
    "    # The following condition is commented out but could be used to drop rows with any -1 values\n",
    "    # if -1 in baseline_mov.loc[i, :].values:\n",
    "    #     index_to_drop.append(i)\n",
    "\n",
    "# Drop the identified rows and reset the DataFrame index\n",
    "baseline_mov_dist = baseline_mov.drop(index_to_drop, axis=0).reset_index(drop=True)\n",
    "\n",
    "# Create a copy of the filtered baseline movement DataFrame for imputation by spatial interation model\n",
    "baseline_mov_imput_sim = baseline_mov_dist.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2baf42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary mapping class indices to pairs of density labels\n",
    "n_class = 0\n",
    "n_labels_density = 5\n",
    "n_class_labels_density = {}\n",
    "\n",
    "# Iterate over all combinations of density labels (0 to 4)\n",
    "for i in range(n_labels_density):\n",
    "    for j in range(n_labels_density):\n",
    "        n_class_labels_density[n_class] = [i, j]\n",
    "        n_class += 1\n",
    "\n",
    "# Create a dictionary mapping class indices to pairs of RDI labels (starting from 1)\n",
    "n_class = 0\n",
    "n_labels_rdi = 5\n",
    "n_class_labels_rdi = {}\n",
    "\n",
    "# Iterate over all combinations of RDI labels (1 to 5)\n",
    "for i in range(n_labels_rdi):\n",
    "    for j in range(n_labels_rdi):\n",
    "        n_class_labels_rdi[n_class] = [i + 1, j + 1]\n",
    "        n_class += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64824dfd",
   "metadata": {},
   "source": [
    "# Generate table explanatory variables for Spatial Interaction Model (SIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b1f111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Uncomment this block only if you want to generate a table of explanatory variables for SIM\n",
    "\n",
    "# # Initialise lists to store variables for the explanatory dataset\n",
    "# y = []                 # Movement counts or dependent variable\n",
    "# O = []                 # Origin grid cell IDs\n",
    "# D = []                 # Destination grid cell IDs\n",
    "# pop_O = []             # Population at origin for each weekday\n",
    "# pop_D = []             # Population at destination for each weekday\n",
    "# wday = []              # Weekday index (as string)\n",
    "# OD_class_density = []  # Combined origin-destination density class\n",
    "# OD_class_rdi = []      # Combined origin-destination RDI class\n",
    "# dist = []              # Distance between origin and destination centroids\n",
    "# index_m1_flat = []     # Indices where movement count = -1 (flat index)\n",
    "# index_m1_pair = []     # Indices where movement count = -1 (pair of [i, weekday])\n",
    "\n",
    "# # Create mapping dictionaries for OD classes based on density and RDI labels\n",
    "# n_class = 0\n",
    "# n_labels_density = 5\n",
    "# n_class_labels_density = {}\n",
    "# for i in range(n_labels_density):\n",
    "#     for j in range(n_labels_density):\n",
    "#         n_class_labels_density[n_class] = [i, j]\n",
    "#         n_class += 1\n",
    "        \n",
    "# n_class = 0\n",
    "# n_labels_rdi = 5\n",
    "# n_class_labels_rdi = {}\n",
    "# for i in range(n_labels_rdi):\n",
    "#     for j in range(n_labels_rdi):\n",
    "#         n_class_labels_rdi[n_class] = [i+1, j+1]  # Labels start from 1 for RDI\n",
    "#         n_class += 1\n",
    "\n",
    "# count = 0  # Flat counter for all observations (across origin-destination pairs and weekdays)\n",
    "\n",
    "# # Loop through all origin-destination pairs in baseline_mov_imput_sim\n",
    "# for i in range(len(baseline_mov_imput_sim)):\n",
    "#     for j in range(7):  # Assuming 7 weekdays indexed 0 to 6\n",
    "#         y.append(baseline_mov_imput_sim.loc[i, str(j)])  # Movement count for day j\n",
    "#         wday.append(str(j))  # Weekday as string\n",
    "        \n",
    "#         # Track indices where movement data is missing (-1)\n",
    "#         if baseline_mov_imput_sim.loc[i, str(j)] == -1:\n",
    "#             index_m1_flat.append(count)\n",
    "#             index_m1_pair.append([i, str(j)])\n",
    "        \n",
    "#         # Retrieve population at origin for the specific weekday\n",
    "#         try:\n",
    "#             pop_origin = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'O']].reset_index(drop=True).loc[0, str(j)]\n",
    "#         except Exception:\n",
    "#             print(i, 'O')  # Debug print if origin population missing\n",
    "#             pop_origin = np.nan\n",
    "#         pop_O.append(pop_origin)\n",
    "#         O.append(baseline_mov_imput_sim.loc[i, 'O'])\n",
    "        \n",
    "#         # Retrieve population at destination for the specific weekday\n",
    "#         try:\n",
    "#             pop_destination = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'D']].reset_index(drop=True).loc[0, str(j)]\n",
    "#         except Exception:\n",
    "#             print(i, 'D')  # Debug print if destination population missing\n",
    "#             pop_destination = np.nan\n",
    "#         pop_D.append(pop_destination)\n",
    "#         D.append(baseline_mov_imput_sim.loc[i, 'D'])\n",
    "        \n",
    "#         # Get density class for origin and destination\n",
    "#         try:\n",
    "#             O_class_density = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'O']].reset_index(drop=True).loc[0, 'class_density']\n",
    "#             D_class_density = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'D']].reset_index(drop=True).loc[0, 'class_density']\n",
    "#         except Exception:\n",
    "#             O_class_density = np.nan\n",
    "#             D_class_density = np.nan\n",
    "        \n",
    "#         # Get RDI class for origin and destination\n",
    "#         try:\n",
    "#             O_class_rdi = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'O']].reset_index(drop=True).loc[0, 'class_rdi']\n",
    "#             D_class_rdi = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'D']].reset_index(drop=True).loc[0, 'class_rdi']\n",
    "#         except Exception:\n",
    "#             O_class_rdi = np.nan\n",
    "#             D_class_rdi = np.nan\n",
    "        \n",
    "#         # Find OD combined density class index\n",
    "#         try:\n",
    "#             OD_class_density.append([key for key, value in n_class_labels_density.items() if value == [O_class_density, D_class_density]][0])\n",
    "#         except Exception:\n",
    "#             OD_class_density.append(np.nan)\n",
    "        \n",
    "#         # Find OD combined RDI class index\n",
    "#         try:\n",
    "#             OD_class_rdi.append([key for key, value in n_class_labels_rdi.items() if value == [O_class_rdi, D_class_rdi]][0])\n",
    "#         except Exception:\n",
    "#             OD_class_rdi.append(np.nan)\n",
    "        \n",
    "#         # Convert to metric CRS for distance calculation\n",
    "#         movcell_baseline_imput_pop = movcell_baseline_imput_pop.to_crs('epsg:3857')\n",
    "        \n",
    "#         # Calculate Euclidean distance between origin and destination centroids\n",
    "#         try:\n",
    "#             O_centroid = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'O']].reset_index(drop=True).loc[0, 'geometry'].centroid\n",
    "#             D_centroid = movcell_baseline_imput_pop[movcell_baseline_imput_pop['FID'] == baseline_mov_imput_sim.loc[i, 'D']].reset_index(drop=True).loc[0, 'geometry'].centroid\n",
    "#             dist.append(O_centroid.distance(D_centroid))\n",
    "#         except Exception:\n",
    "#             dist.append(np.nan)\n",
    "        \n",
    "#         count += 1\n",
    "    \n",
    "#     # Progress print every 1000 OD pairs\n",
    "#     if i % 1000 == 0:\n",
    "#         print(f\"{(i / len(baseline_mov_imput_sim)) * 100:.2f}% processed\")\n",
    "\n",
    "# # Create a boolean list indicating observed movements (True) vs missing (-1) (False)\n",
    "# obs = [True] * len(OD_class_density)\n",
    "# for index in index_m1_flat:\n",
    "#     obs[index] = False\n",
    "\n",
    "# # Combine all lists into a DataFrame\n",
    "# data = pd.DataFrame({\n",
    "#     'y': y,\n",
    "#     'O': O,\n",
    "#     'D': D,\n",
    "#     'OD_class_density': OD_class_density,\n",
    "#     'OD_class_rdi': OD_class_rdi,\n",
    "#     'dist': dist,\n",
    "#     'pop_O': pop_O,\n",
    "#     'pop_D': pop_D,\n",
    "#     'wday': wday,\n",
    "#     'obs': obs\n",
    "# })\n",
    "\n",
    "# # Save to CSV\n",
    "# data.to_csv(wd + f'/data/outputs/{country_short}/baseline/movcell-baseline-mov-dist-with-exo-var.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d7dce",
   "metadata": {},
   "source": [
    "# Import data for SIM (generated in cell above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ef522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the processed data CSV and drop the unnecessary 'Unnamed: 0' column (index column from CSV)\n",
    "data = pd.read_csv(\n",
    "    wd + f'/data/outputs/{country_short}/baseline/movcell-baseline-mov-dist-with-exo-var.csv'\n",
    ").drop(['Unnamed: 0'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cd56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter data to include only observed movements (obs == True) and perform some sanity checks\n",
    "data_test = data[data['obs'] == True]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "data_test = data_test.sort_values(by='y', ascending=True)\n",
    "ax.tick_params(axis='both', which='both', width=0, length=0, color='k', labelsize=25, pad=20)\n",
    "sc = ax.scatter(data_test['pop_O'], data_test['pop_D'], c=np.log10(data_test['y']), cmap='viridis', alpha=0.5, s=60)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim([1, 400000])\n",
    "ax.set_ylim([1, 400000])\n",
    "plt.savefig(wd + f'/plots/mov-data-obs-popOD/mov-data-obs-popOD-{country_short}.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841f9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "data_test = data_test.sort_values(by='y', ascending=True)\n",
    "ax.tick_params(axis='both', which='both', width=0, length=0, color='k', labelsize=25, pad=20)\n",
    "sc = ax.scatter(data_test['pop_O'], data_test['pop_D'], c=np.log10(data_test['y']), cmap='viridis', alpha=1, s=60)\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "cbar = plt.colorbar(sc, ax=ax, shrink=0.5)\n",
    "cbar.set_label('Log(y)', fontsize=20)\n",
    "cbar.ax.tick_params(labelsize=15, length=0)\n",
    "ax.set_xlim([9, 500000])\n",
    "ax.set_ylim([9, 500000])\n",
    "# plt.savefig(wd + f'/plots/mov-data-obs-popOD/mov-data-obs-popOD-{country_short}-legend.png', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sc = ax.scatter(data_test['pop_O'], data_test['y'], c=np.log(data_test['pop_D']), cmap='viridis', alpha=0.25)\n",
    "x = np.linspace(min(data_test['pop_O']), max(data_test['pop_O']))\n",
    "ax.plot(x, x, lw=2, color='darkred', linestyle=':')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "# plt.savefig(wd + f'/plots/mov-data-obs-popO-flow/mov-data-obs-popO-flow-{country_short}.pdf', bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical variables to dummy/indicator variables\n",
    "dummies_wday = pd.get_dummies(data['wday'], drop_first=True, dtype=int)\n",
    "\n",
    "dummies_OD_class_density = pd.get_dummies(data['OD_class_density'], drop_first=True, dtype=int)\n",
    "dummies_OD_class_rdi = pd.get_dummies(data['OD_class_rdi'], drop_first=True, dtype=int)\n",
    "\n",
    "# Rename columns of dummies_OD_class_density to append suffix '_class_density'\n",
    "dummies_OD_class_density = dummies_OD_class_density.rename(\n",
    "    columns={col: str(int(col)) + '_class_density' for col in dummies_OD_class_density.columns}\n",
    ")\n",
    "\n",
    "# Rename columns of dummies_OD_class_rdi to append suffix '_class_rdi'\n",
    "dummies_OD_class_rdi = dummies_OD_class_rdi.rename(\n",
    "    columns={col: str(int(col)) + '_class_rdi' for col in dummies_OD_class_rdi.columns}\n",
    ")\n",
    "\n",
    "# Select relevant numeric columns from the original data\n",
    "X = data[['dist', 'pop_O', 'pop_D', 'obs', 'y']]\n",
    "\n",
    "# Concatenate weekday dummies with the selected numeric columns\n",
    "X = pd.concat([dummies_wday, X], axis=1)\n",
    "\n",
    "# Alternative concatenations commented out:\n",
    "# - including other dummy variables for origin (O) and OD classes\n",
    "# X = pd.concat([dummies_wday, X, dummies_O, dummies_OD_class_density, dummies_OD_class_rdi], axis=1)\n",
    "# X = pd.concat([dummies_wday, X, dummies_O], axis=1)\n",
    "# X = pd.concat([X], axis=1)\n",
    "\n",
    "# Add constant term for intercept in regression models\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Filter rows where 'dist', 'pop_O', and 'pop_D' are greater than zero\n",
    "X = X[X['dist'] > 0].reset_index(drop=True)\n",
    "X = X[X['pop_O'] > 0].reset_index(drop=True)\n",
    "X = X[X['pop_D'] > 0].reset_index(drop=True)\n",
    "\n",
    "# Subset observed data where 'obs' == True\n",
    "X_obs = X[X['obs'] == True].reset_index(drop=True)\n",
    "\n",
    "# Extract target variable 'y' for observed data\n",
    "y_obs = X_obs['y']\n",
    "\n",
    "# Drop columns not needed for modeling from observed dataset\n",
    "X_obs = X_obs.drop(['obs', 'y'], axis=1)\n",
    "\n",
    "# Prepare prediction dataset by dropping 'obs' and 'y' columns from full dataset\n",
    "X_predict = X.drop(['obs', 'y'], axis=1)\n",
    "\n",
    "# Drop rows with any missing values (NaNs) in the observed dataset\n",
    "X_obs_dropna = X_obs.dropna()\n",
    "\n",
    "# Optional filters commented out: only keep rows where population columns > 1\n",
    "# X_obs_dropna = X_obs_dropna[X_obs_dropna['pop_O'] > 1]\n",
    "# X_obs_dropna = X_obs_dropna[X_obs_dropna['pop_D'] > 1]\n",
    "\n",
    "# Align target variable 'y_obs' with the filtered dataset using indices\n",
    "y_obs_dropna = y_obs.iloc[X_obs_dropna.index].reset_index(drop=True)\n",
    "\n",
    "# Reset index of the filtered dataset for clean indexing\n",
    "X_obs_dropna = X_obs_dropna.reset_index(drop=True)\n",
    "\n",
    "# Apply natural logarithm transformation to population and distance variables\n",
    "X_obs_dropna['pop_O'] = np.log(X_obs_dropna['pop_O'])\n",
    "X_obs_dropna['pop_D'] = np.log(X_obs_dropna['pop_D'])\n",
    "X_obs_dropna['dist'] = np.log(X_obs_dropna['dist'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68f9cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the target variable to create sampling weights:\n",
    "# Higher target values will have higher probability of being sampled.\n",
    "# This helps avoid the sample being dominated by smaller values (e.g., below 10),\n",
    "# which you can see by comparing histograms below.\n",
    "prob_normalized = y_obs_dropna / y_obs_dropna.sum()\n",
    "\n",
    "# Create a DataFrame for sampling purposes\n",
    "sample_y_obs_dropna = pd.DataFrame({'y_obs_dropna': y_obs_dropna})\n",
    "\n",
    "# Sample 3000 rows without replacement, weighted by the normalized target values\n",
    "sample_y_obs_dropna = sample_y_obs_dropna.sample(\n",
    "    n=3000,\n",
    "    weights=prob_normalized,\n",
    "    replace=False\n",
    ")\n",
    "\n",
    "# Plot histograms to compare the distribution of sampled vs. original target values\n",
    "fig, ax = plt.subplots()\n",
    "ax.hist(sample_y_obs_dropna['y_obs_dropna'], bins=50, color='blue', alpha=0.5, density=True, label='Sampled')\n",
    "ax.hist(y_obs_dropna, bins=50, color='red', alpha=0.5, density=True, label='Original')\n",
    "ax.set_yscale('log')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897c818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different options form movel, best one is GLM\n",
    "\n",
    "X_obs_dropna = X_obs_dropna.iloc[sample_y_obs_dropna.index]\n",
    "y_obs_dropna = y_obs_dropna.iloc[sample_y_obs_dropna.index]\n",
    "\n",
    "\n",
    "# def sse_log_a(a, Y, X):\n",
    "#     log_aY = np.log(a * Y)\n",
    "#     model = sm.GLM(log_aY, X, family=sm.families.NegativeBinomial(alpha=1))\n",
    "#     res = model.fit()\n",
    "#     opt = res.residual\n",
    "#     print(opt)\n",
    "#     return opt\n",
    "\n",
    "# result = scipy.optimize.minimize(sse_log_a, x0=[12], args=(y_obs_dropna, X_obs_dropna), method='BFGS')\n",
    "# a_estimated = result.x[0]\n",
    "# print(f\"Estimated value of a: {a_estimated}\")\n",
    "\n",
    "# log_aY = np.log(a_estimated * y_obs_dropna)\n",
    "# model = sm.GLM(log_aY, X_obs_dropna, family=sm.families.NegativeBinomial(alpha=1))\n",
    "# res = model.fit()\n",
    "# print(res.summary())\n",
    "\n",
    "model = sm.GLM(np.log(y_obs_dropna), X_obs_dropna, family=sm.families.Gaussian())\n",
    "res = model.fit()\n",
    "print(res.summary())\n",
    "\n",
    "# model = sm.RLM(np.log(y_obs_dropna), X_obs_dropna, M=sm.robust.norms.HuberT())\n",
    "# res = model.fit()\n",
    "# print(res.summary())\n",
    "\n",
    "# model = sm.GLM(np.log(y_obs_dropna), X_obs_dropna, family=sm.families.Gaussian(sm.families.links.identity()))\n",
    "# res = model.fit(cov_type='HC3')\n",
    "# print(res.summary())\n",
    "\n",
    "# data_quantreg = X_obs_dropna.copy()\n",
    "# data_quantreg['Y'] = np.log(y_obs_dropna)\n",
    "# data_quantreg.columns = ['const', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'dist', 'pop_O', 'pop_D', 'Y']\n",
    "# model = smf.quantreg('Y ~ const + d1 + d2 + d3 + d4 + d5 + d6 + dist + pop_O + pop_D', data=data_quantreg)\n",
    "# res = model.fit(q=0.75)\n",
    "# print(res.summary())\n",
    "\n",
    "# model = sm.GLM(np.log(10*y_obs_dropna), X_obs_dropna, family=sm.families.Poisson())\n",
    "# res = model.fit()\n",
    "# llf_Poisson = res.llf\n",
    "\n",
    "# model = sm.GLM(y_obs_dropna, X_obs_dropna, family=sm.families.NegativeBinomial(alpha=1))\n",
    "# res = model.fit()\n",
    "# llf_NB = res.llf\n",
    "\n",
    "# # log likelihood ratio test\n",
    "# logratio = -2 * (llf_Poisson - llf_NB)\n",
    "# df = 1 # given the difference in dof\n",
    "# # compute the p-value\n",
    "# pvalue = 1 - scipy.stats.chi2(df).cdf(logratio) # since Λ follows χ2\n",
    "# print('logratio', logratio, 'p-value', pvalue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d143d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Drop rows with missing values in the prediction dataset and reset index\n",
    "X_predict_dropna = X_predict.dropna().reset_index(drop=True)\n",
    "\n",
    "# Apply log transformation to the relevant predictor variables\n",
    "X_predict_dropna['pop_O'] = np.log(X_predict_dropna['pop_O'])\n",
    "X_predict_dropna['pop_D'] = np.log(X_predict_dropna['pop_D'])\n",
    "X_predict_dropna['dist'] = np.log(X_predict_dropna['dist'])\n",
    "\n",
    "# Make a copy of the cleaned dataset for quantile regression\n",
    "X_predict_dropna_quantreg = X_predict_dropna.copy()\n",
    "\n",
    "# Use the fitted model to predict on the cleaned, log-transformed dataset\n",
    "y_predict = res.predict(X_predict_dropna)\n",
    "\n",
    "# Rename columns to match expected names for quantile regression model (if needed)\n",
    "# X_predict_dropna_quantreg.columns = ['const', 'd1', 'd2', 'd3', 'd4', 'd5', 'd6', 'dist', 'pop_O', 'pop_D']\n",
    "# Alternative prediction call for quantile regression dataset (commented out)\n",
    "# y_predict = res.predict(X_predict_dropna_quantreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3f9e5",
   "metadata": {},
   "source": [
    "# Save values imputed according to SIM in standard and long format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e94d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the target variable to store imputed values\n",
    "data['y_imput'] = data['y']\n",
    "\n",
    "# Initialise a counter for predictions\n",
    "j = 0\n",
    "\n",
    "# Iterate over the rows to replace '-1' values in 'y_imput' with predictions\n",
    "for i in range(len(data)):\n",
    "    if data.loc[i, 'y_imput'] == -1:\n",
    "        # Impute missing values using the exponentiated predicted log-values\n",
    "        data.loc[i, 'y_imput'] = np.exp(y_predict[j])\n",
    "        j += 1\n",
    "    else:\n",
    "        # Keep original values if not '-1'\n",
    "        data.loc[i, 'y_imput'] = data.loc[i, 'y']\n",
    "\n",
    "# Optional: save the updated data to CSV \n",
    "data.to_csv(wd + '/data/outputs/' + country_short + '/baseline/movcell-baseline-imput-mov-dist-with-exo-var-sample.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6f3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise counter for indexing into data['y_imput']\n",
    "k = 0\n",
    "\n",
    "# Make a copy of the baseline_mov_dist dataframe to perform imputations\n",
    "baseline_mov_imput = baseline_mov_dist.copy()\n",
    "\n",
    "# Iterate over rows and columns (columns named '0' to '6') to replace -1 with imputed values\n",
    "for i in range(len(baseline_mov_imput)):\n",
    "    for j in range(7):\n",
    "        col = str(j)\n",
    "        if baseline_mov_imput.loc[i, col] == -1:\n",
    "            baseline_mov_imput.loc[i, col] = data.loc[k, 'y_imput']\n",
    "        k += 1\n",
    "\n",
    "# Optional: save the imputed DataFrame to CSV (commented out)\n",
    "baseline_mov_imput.to_csv(wd + f'/data/outputs/{country_short}/baseline/movcell-baseline-imput-mov-dist-with-exo-var-flatten-sample.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7103d5c",
   "metadata": {},
   "source": [
    "# A few tests to validate estiamted baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9352040b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict_test = pd.DataFrame({'y_predict_test': res.predict(X_obs_dropna)})\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "\n",
    "ax.tick_params(axis = 'both', which = 'both', width = 0, length = 0, color = 'k', labelsize = 25, pad=20)\n",
    "\n",
    "ax.scatter(y_obs_dropna, np.exp(y_predict_test['y_predict_test']), alpha=0.1, s=60, color='darkblue')\n",
    "\n",
    "pearson = scipy.stats.pearsonr(y_obs_dropna, np.exp(y_predict_test['y_predict_test']))\n",
    "R = str(round(pearson.statistic,3))\n",
    "pvalue = str(round(pearson.pvalue,2))\n",
    "\n",
    "if float(pvalue) < 0.05:\n",
    "    pvalue = '< 0.05'\n",
    "ax.text(0.04, 0.87, 'R = ' + R + '\\np-value = ' + pvalue, transform=ax.transAxes, size=25)\n",
    "\n",
    "ax.set_xlim([6,50000])\n",
    "ax.set_ylim([0.6, 6000])\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.savefig(wd + '/plots/mov-data-obs-pred-flow/mov-data-obs-pred-flow-' + country_short + '.png', bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56abd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_ovalue = 0\n",
    "count_uvalue = 0\n",
    "value = 10\n",
    "for i in range(len(y_predict)):\n",
    "    if np.exp(y_predict[i]) < value:\n",
    "        count_uvalue += 1\n",
    "    elif np.exp(y_predict[i]) >= value:\n",
    "        count_ovalue += 1\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(data[data['y']==-1]['pop_O'], data[data['y']==-1]['y_imput'], zorder=2, color='red', s=5, alpha=0.002)\n",
    "ax.scatter(np.exp(X_obs_dropna['pop_O']), np.exp(y_predict_test), zorder=5, color='gold', s=5, alpha=0.4)\n",
    "ax.scatter(np.exp(X_obs_dropna['pop_O']), y_obs_dropna, zorder= 4, color='blue', s=5, alpha=0.3)\n",
    "\n",
    "# ax.set_xlim([-50000, 300000])\n",
    "# ax.set_ylim([-100, 200])\n",
    "\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f83218",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.scatter(X_obs['pop_O'], X_obs['pop_D'], color='blue', s=5, alpha=0.3, label='Observed')\n",
    "ax.scatter(data[data['y'] == -1]['pop_O'], data[data['y'] == -1]['pop_D'], color='red', s=5, alpha=0.3, label='Missing')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b40e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(data[data['y']>-1]['y_imput'], bins=120, density=True, color='blue', alpha=0.5, zorder=3)\n",
    "ax.hist(data[data['y']==-1]['y_imput'], bins=120, density=True, color='red', alpha=0.6, zorder=2)\n",
    "ax.set_yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b7fdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "# ax.hist(data[data['y']>-1]['y_imput'], bins=120, density=True, color='blue', alpha=0.5, zorder=3)\n",
    "ax.hist(np.exp(y_predict_test), bins=120, density=True, color='red', alpha=0.6, zorder=2)\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
